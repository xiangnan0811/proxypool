2020-07-17 10:35:04.933 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (48960), thread 'MainThread' (53192):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001C7459AF798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001C745989288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001C7459878B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001C74774CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001C747453AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001C745985D48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001C7472C1288>
        ©¦        ©¸ <public.dali66.Daili66Crawler object at 0x000001C747765888>
        ©¸ Proxy(host='101.71.40.163', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.66ip.cn/1.html'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001C7472C11F8>
           ©¸ <public.dali66.Daili66Crawler object at 0x000001C747765888>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.dali66.Daili66Crawler object at 0x000001C747765888>, 'http://www.66ip.cn/1.html')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001C7472C1168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001C746E48288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 10:35:16.673 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (48960), thread 'MainThread' (53192):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001C7459AF798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001C745989288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001C7459878B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001C74774CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001C747453AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001C745985D48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001C7472C1288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x000001C7477659C8>
        ©¸ Proxy(host='95.155.8.196', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001C7472C11F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x000001C7477659C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x000001C7477659C8>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001C7472C1168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001C746E48288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 10:39:17.346 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (60096), thread 'MainThread' (54808):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001CAE7370798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001CAE734A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001CAE73488B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001CAE910CCC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001CAE8E13AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001CAE7346D08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001CAE8C81288>
        ©¦        ©¸ <public.dali66.Daili66Crawler object at 0x000001CAE9126788>
        ©¸ Proxy(host='101.71.40.163', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.66ip.cn/1.html'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001CAE8C811F8>
           ©¸ <public.dali66.Daili66Crawler object at 0x000001CAE9126788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.dali66.Daili66Crawler object at 0x000001CAE9126788>, 'http://www.66ip.cn/1.html')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001CAE8C81168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001CAE8807288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 10:39:59.974 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (58024), thread 'MainThread' (61284):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001865E09F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001865E07A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001865E0788B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001865FE3BD08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001865FB43AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001865E075D48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001865F9B0288>
        ©¦        ©¸ <public.dali66.Daili66Crawler object at 0x000001865FE557C8>
        ©¸ Proxy(host='101.71.40.163', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.66ip.cn/1.html'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001865F9B01F8>
           ©¸ <public.dali66.Daili66Crawler object at 0x000001865FE557C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.dali66.Daili66Crawler object at 0x000001865FE557C8>, 'http://www.66ip.cn/1.html')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001865F9B0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001865F538288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 10:40:15.015 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (58024), thread 'MainThread' (61284):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001865E09F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001865E07A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001865E0788B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001865FE3BD08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001865FB43AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001865E075D48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001865F9B0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x000001865FE55908>
        ©¸ Proxy(host='95.155.8.196', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001865F9B01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x000001865FE55908>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x000001865FE55908>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001865F9B0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001865F538288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 10:44:18.387 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (58024), thread 'MainThread' (61284):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001865E09F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001865E07A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001865E0788B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001865FE3BD08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001865FB43AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001865E075D48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001865F9B0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001865FE55948>
        ©¸ Proxy(host='95.155.8.196', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/1/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001865F9B01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001865FE55948>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001865FE55948>, 'https://www.kuaidaili.com/free/inha/1/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001865F9B0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001865F538288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 10:44:31.700 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (58024), thread 'MainThread' (61284):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001865E09F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001865E07A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001865E0788B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001865FE3BD08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001865FB43AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001865E075D48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001865F9B0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001865FE559C8>
        ©¸ Proxy(host='45.137.217.27', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000001865FB439D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001865FE559C8>
        ©¸ Proxy(host='45.137.217.27', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000001865FA60D08>
              ©¸ <Element tr at 0x1865fe55a88>

IndexError: list index out of range
2020-07-17 10:48:35.473 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (58024), thread 'MainThread' (61284):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001865E09F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001865E07A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001865E0788B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001865FE3BD08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001865FB43AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001865E075D48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001865F9B0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001865FE55A08>
        ©¸ Proxy(host='45.137.217.27', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001865F9B01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001865FE55A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001865FE55A08>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001865F9B0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001865F538288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 10:50:21.445 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (58024), thread 'MainThread' (61284):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001865E09F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001865E07A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001865E0788B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001865FE3BD08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001865FB43AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001865E075D48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001865F9B0288>
        ©¦        ©¸ <public.dali66.Daili66Crawler object at 0x000001865FE557C8>
        ©¸ Proxy(host='101.71.40.163', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.66ip.cn/1.html'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001865F9B01F8>
           ©¸ <public.dali66.Daili66Crawler object at 0x000001865FE557C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.dali66.Daili66Crawler object at 0x000001865FE557C8>, 'http://www.66ip.cn/1.html')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001865F9B0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001865F538288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 10:50:30.692 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (58024), thread 'MainThread' (61284):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001865E09F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001865E07A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001865E0788B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001865FE3BD08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001865FB43AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001865E075D48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001865F9B0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x000001865FE55908>
        ©¸ Proxy(host='95.155.8.196', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001865F9B01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x000001865FE55908>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x000001865FE55908>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001865F9B0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001865F538288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 10:54:34.024 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (58024), thread 'MainThread' (61284):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001865E09F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001865E07A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001865E0788B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001865FE3BD08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001865FB43AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001865E075D48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001865F9B0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001865FE55948>
        ©¸ Proxy(host='95.155.8.196', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/1/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001865F9B01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001865FE55948>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001865FE55948>, 'https://www.kuaidaili.com/free/inha/1/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001865F9B0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001865F538288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 10:54:58.387 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (58024), thread 'MainThread' (61284):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001865E09F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001865E07A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001865E0788B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001865FE3BD08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001865FB43AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001865E075D48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001865F9B0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001865FE559C8>
        ©¸ Proxy(host='183.167.217.152', port=63000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000001865FB439D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001865FE559C8>
        ©¸ Proxy(host='183.167.217.152', port=63000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000001865FA60D08>
              ©¸ <Element tr at 0x1865ff28788>

IndexError: list index out of range
2020-07-17 10:59:01.734 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (58024), thread 'MainThread' (61284):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001865E09F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001865E07A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001865E0788B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001865FE3BD08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001865FB43AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001865E075D48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001865F9B0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001865FE55A08>
        ©¸ Proxy(host='183.167.217.152', port=63000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001865F9B01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001865FE55A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001865FE55A08>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001865F9B0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001865F538288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 11:00:50.183 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (58024), thread 'MainThread' (61284):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001865E09F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001865E07A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001865E0788B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001865FE3BD08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001865FB43AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001865E075D48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001865F9B0288>
        ©¦        ©¸ <public.dali66.Daili66Crawler object at 0x000001865FE557C8>
        ©¸ Proxy(host='101.71.40.163', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.66ip.cn/1.html'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001865F9B01F8>
           ©¸ <public.dali66.Daili66Crawler object at 0x000001865FE557C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.dali66.Daili66Crawler object at 0x000001865FE557C8>, 'http://www.66ip.cn/1.html')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001865F9B0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001865F538288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 11:01:01.183 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (58024), thread 'MainThread' (61284):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001865E09F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001865E07A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001865E0788B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001865FE3BD08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001865FB43AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001865E075D48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001865F9B0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x000001865FE55908>
        ©¸ Proxy(host='95.155.8.196', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001865F9B01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x000001865FE55908>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x000001865FE55908>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001865F9B0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001865F538288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 11:05:01.436 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (66180), thread 'MainThread' (66252):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000002554AC8F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000002554AC6B288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000002554AC688B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000002554CA2DD48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000002554C733AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000002554AC69108>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000002554C5A1288>
        ©¦        ©¸ <public.dali66.Daili66Crawler object at 0x000002554CA477C8>
        ©¸ Proxy(host='149.129.61.224', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.66ip.cn/1.html'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000002554C5A11F8>
           ©¸ <public.dali66.Daili66Crawler object at 0x000002554CA477C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.dali66.Daili66Crawler object at 0x000002554CA477C8>, 'http://www.66ip.cn/1.html')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000002554C5A1168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000002554C128288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 11:05:08.603 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (66180), thread 'MainThread' (66252):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000002554AC8F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000002554AC6B288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000002554AC688B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000002554CA2DD48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000002554C733AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000002554AC69108>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000002554C5A1288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x000002554CA47908>
        ©¸ Proxy(host='95.155.8.196', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000002554C5A11F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x000002554CA47908>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x000002554CA47908>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000002554C5A1168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000002554C128288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 11:09:11.920 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (66180), thread 'MainThread' (66252):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000002554AC8F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000002554AC6B288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000002554AC688B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000002554CA2DD48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000002554C733AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000002554AC69108>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000002554C5A1288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000002554CA47948>
        ©¸ Proxy(host='95.155.8.196', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/1/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000002554C5A11F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000002554CA47948>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000002554CA47948>, 'https://www.kuaidaili.com/free/inha/1/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000002554C5A1168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000002554C128288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 11:09:23.368 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (66180), thread 'MainThread' (66252):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000002554AC8F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000002554AC6B288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000002554AC688B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000002554CA2DD48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000002554C733AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000002554AC69108>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000002554C5A1288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000002554CA479C8>
        ©¸ Proxy(host='139.217.110.76', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000002554C7339D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000002554CA479C8>
        ©¸ Proxy(host='139.217.110.76', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000002554C652D08>
              ©¸ <Element tr at 0x2554cb1cdc8>

IndexError: list index out of range
2020-07-17 11:11:32.821 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (38392), thread 'MainThread' (66356):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001AB8D500798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001AB8D4DA288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001AB8D4D88B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001AB8F29BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001AB8EFA3AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001AB8D4D5D48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001AB8EE11288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001AB8F2B5688>
        ©¸ Proxy(host='60.188.251.80', port=3000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001AB8EE111F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001AB8F2B5688>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x000001AB8F2B5688>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001AB8EE11168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001AB8E997288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 11:15:07.386 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (2796), thread 'MainThread' (60892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000002A71505F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000002A715038288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000002A7150378B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000002A716DFCCC8>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000002A716DC4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000002A715034D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000002A716DBC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000002A715034D88>

TypeError: 'NoneType' object is not iterable
2020-07-17 11:16:19.521 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (17420), thread 'MainThread' (48684):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001997D72F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001997D70A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001997D7088B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001997F4CBB88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001997F1D1AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001997D706CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001997F041288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001997F4E75C8>
        ©¸ Proxy(host='60.188.251.80', port=3000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001997F0411F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001997F4E75C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x000001997F4E75C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001997F041168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001997EBC6288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 11:16:26.737 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (17420), thread 'MainThread' (48684):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001997D72F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001997D70A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001997D7088B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001997F4CBB88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001997F1D1AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001997D706CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001997F041288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x000001997F4E7748>
        ©¸ Proxy(host='95.155.8.196', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001997F0411F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x000001997F4E7748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x000001997F4E7748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001997F041168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001997EBC6288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 11:16:27.832 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (17420), thread 'MainThread' (48684):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001997D72F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001997D70A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001997D7088B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001997F4CBB88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001997F1D1AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001997D706CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001997F041288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001997F4E7788>
        ©¸ Proxy(host='121.232.148.79', port='9000')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001997F0411F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001997F4E7788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001997F4E7788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001997F041168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001997EBC6288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 11:16:40.855 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (17420), thread 'MainThread' (48684):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001997D72F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001997D70A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001997D7088B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001997F4CBB88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001997F1D1AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001997D706CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001997F041288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001997F4E7808>
        ©¸ Proxy(host='110.189.152.86', port=40698)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000001997F1D19D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001997F4E7808>
        ©¸ Proxy(host='110.189.152.86', port=40698)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000001997F0F1D08>
              ©¸ <Element tr at 0x1997f615608>

IndexError: list index out of range
2020-07-17 11:17:44.179 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (17420), thread 'MainThread' (48684):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001997D72F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001997D70A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001997D7088B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001997F4CBB88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001997F1D1AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001997D706CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001997F041288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001997F4E7848>
        ©¸ Proxy(host='110.189.152.86', port=40698)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001997F0411F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001997F4E7848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001997F4E7848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001997F041168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001997EBC6288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 11:48:01.180 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (1684), thread 'MainThread' (68432):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000023C56C7F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x0000023C56C5B288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x0000023C56C588B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x0000023C58A1BD48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x0000023C589E3948>
    ©¸ <proxypool.processors.tester.Tester object at 0x0000023C56C59088>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x0000023C589DB438>
             ©¸ <proxypool.processors.tester.Tester object at 0x0000023C56C59088>

TypeError: 'NoneType' object is not iterable
2020-07-17 11:57:29.210 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (1684), thread 'MainThread' (68432):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000023C56C7F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x0000023C56C5B288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x0000023C56C588B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x0000023C58A1BD48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x0000023C589E3948>
    ©¸ <proxypool.processors.tester.Tester object at 0x0000023C56C59088>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x0000023C589DB438>
             ©¸ <proxypool.processors.tester.Tester object at 0x0000023C56C59088>

TypeError: 'NoneType' object is not iterable
2020-07-17 12:01:29.248 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (1684), thread 'MainThread' (68432):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000023C56C7F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x0000023C56C5B288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x0000023C56C588B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x0000023C58A1BD48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x0000023C589E3948>
    ©¸ <proxypool.processors.tester.Tester object at 0x0000023C56C59088>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x0000023C589DB438>
             ©¸ <proxypool.processors.tester.Tester object at 0x0000023C56C59088>

TypeError: 'NoneType' object is not iterable
2020-07-17 12:05:18.176 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (1684), thread 'MainThread' (68432):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000023C56C7F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x0000023C56C5B288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x0000023C56C588B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x0000023C58A1BD48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x0000023C589E3948>
    ©¸ <proxypool.processors.tester.Tester object at 0x0000023C56C59088>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x0000023C589DB438>
             ©¸ <proxypool.processors.tester.Tester object at 0x0000023C56C59088>

TypeError: 'NoneType' object is not iterable
2020-07-17 12:08:45.199 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (1684), thread 'MainThread' (68432):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000023C56C7F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x0000023C56C5B288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x0000023C56C588B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x0000023C58A1BD48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x0000023C589E3948>
    ©¸ <proxypool.processors.tester.Tester object at 0x0000023C56C59088>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x0000023C589DB438>
             ©¸ <proxypool.processors.tester.Tester object at 0x0000023C56C59088>

TypeError: 'NoneType' object is not iterable
2020-07-17 12:11:50.229 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (1684), thread 'MainThread' (68432):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000023C56C7F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x0000023C56C5B288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x0000023C56C588B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x0000023C58A1BD48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x0000023C589E3948>
    ©¸ <proxypool.processors.tester.Tester object at 0x0000023C56C59088>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x0000023C589DB438>
             ©¸ <proxypool.processors.tester.Tester object at 0x0000023C56C59088>

TypeError: 'NoneType' object is not iterable
2020-07-17 12:14:33.251 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (1684), thread 'MainThread' (68432):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000023C56C7F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x0000023C56C5B288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x0000023C56C588B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x0000023C58A1BD48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x0000023C589E3948>
    ©¸ <proxypool.processors.tester.Tester object at 0x0000023C56C59088>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x0000023C589DB438>
             ©¸ <proxypool.processors.tester.Tester object at 0x0000023C56C59088>

TypeError: 'NoneType' object is not iterable
2020-07-17 12:16:56.543 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (1684), thread 'MainThread' (68432):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000023C56C7F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x0000023C56C5B288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x0000023C56C588B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x0000023C58A1BD48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x0000023C589E3948>
    ©¸ <proxypool.processors.tester.Tester object at 0x0000023C56C59088>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x0000023C589DB438>
             ©¸ <proxypool.processors.tester.Tester object at 0x0000023C56C59088>

TypeError: 'NoneType' object is not iterable
2020-07-17 12:19:06.241 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (1684), thread 'MainThread' (68432):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000023C56C7F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x0000023C56C5B288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x0000023C56C588B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x0000023C58A1BD48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x0000023C589E3948>
    ©¸ <proxypool.processors.tester.Tester object at 0x0000023C56C59088>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x0000023C589DB438>
             ©¸ <proxypool.processors.tester.Tester object at 0x0000023C56C59088>

TypeError: 'NoneType' object is not iterable
2020-07-17 12:23:04.261 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (1684), thread 'MainThread' (68432):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000023C56C7F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x0000023C56C5B288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x0000023C56C588B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x0000023C58A1BD48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x0000023C589E3948>
    ©¸ <proxypool.processors.tester.Tester object at 0x0000023C56C59088>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x0000023C589DB438>
             ©¸ <proxypool.processors.tester.Tester object at 0x0000023C56C59088>

TypeError: 'NoneType' object is not iterable
2020-07-17 12:29:32.253 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (1684), thread 'MainThread' (68432):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000023C56C7F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x0000023C56C5B288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x0000023C56C588B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x0000023C58A1BD48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x0000023C589E3948>
    ©¸ <proxypool.processors.tester.Tester object at 0x0000023C56C59088>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x0000023C589DB438>
             ©¸ <proxypool.processors.tester.Tester object at 0x0000023C56C59088>

TypeError: 'NoneType' object is not iterable
2020-07-17 16:42:30.744 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='123.195.152.139', port=30701)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:42:39.671 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='222.189.190.160', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:42:40.691 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='60.167.102.241', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:42:59.109 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='58.87.98.150', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='58.87.98.150', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f74db9c8>

IndexError: list index out of range
2020-07-17 16:44:02.362 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='58.87.98.150', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:45:46.428 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='123.195.152.139', port=30701)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:45:53.099 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='222.189.190.160', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:45:54.013 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='125.108.78.124', port='9000')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:45:55.123 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>
        ©¸ Proxy(host='175.43.33.36', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/3'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>, 'http://www.nimadaili.com/gaoni/3')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:46:02.577 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='58.87.98.150', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='58.87.98.150', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f74d5808>

IndexError: list index out of range
2020-07-17 16:47:05.827 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='58.87.98.150', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:48:49.225 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='123.195.152.139', port=30701)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:48:53.873 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='222.189.190.160', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:48:54.787 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='125.108.78.124', port='9000')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:48:55.974 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>
        ©¸ Proxy(host='45.250.226.56', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/3'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>, 'http://www.nimadaili.com/gaoni/3')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:49:05.326 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='58.87.98.150', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='58.87.98.150', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f74e3608>

IndexError: list index out of range
2020-07-17 16:50:08.569 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='58.87.98.150', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:52:09.148 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='123.195.152.139', port=30701)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:52:15.139 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='222.189.190.160', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:52:16.062 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='125.108.78.124', port='9000')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:52:35.265 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='27.128.247.85', port=808)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='27.128.247.85', port=808)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f74d4148>

IndexError: list index out of range
2020-07-17 16:53:38.485 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='27.128.247.85', port=808)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:55:25.177 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='123.195.152.139', port=30701)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:55:31.966 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='222.189.190.160', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:55:32.919 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='125.108.78.124', port='9000')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:55:34.145 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>
        ©¸ Proxy(host='175.43.33.36', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/3'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>, 'http://www.nimadaili.com/gaoni/3')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:56:38.729 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='27.128.247.85', port=808)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='27.128.247.85', port=808)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f74ef108>

IndexError: list index out of range
2020-07-17 16:57:41.985 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='27.128.247.85', port=808)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:59:27.893 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='123.195.152.139', port=30701)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:59:32.621 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='222.189.190.160', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:59:33.610 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='125.108.78.124', port='9000')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 16:59:35.641 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>
        ©¸ Proxy(host='45.76.121.90', port=32202)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/5'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>, 'http://www.nimadaili.com/gaoni/5')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 17:00:58.273 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='115.223.7.110', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 17:02:41.941 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='117.69.13.61', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 17:02:47.621 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='222.189.190.160', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 17:02:48.556 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='125.108.78.124', port='9000')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 17:02:50.211 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>
        ©¸ Proxy(host='175.43.33.36', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/4'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>, 'http://www.nimadaili.com/gaoni/4')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 17:02:54.031 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='159.138.22.112', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='159.138.22.112', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f74e37c8>

IndexError: list index out of range
2020-07-17 17:03:57.264 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='159.138.22.112', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 17:06:03.018 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='117.69.13.61', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 17:06:11.162 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='222.189.190.160', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 17:06:12.130 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='125.108.78.124', port='9000')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 17:06:22.071 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='159.138.21.170', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://ip.ihuan.me/address/5Lit5Zu9.html?page=4ce63706'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>, 'https://ip.ihuan.me/address/5Lit5Zu9.html?page=4...
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 17:07:25.315 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='159.138.21.170', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 17:09:07.590 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
                 ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
                 ©¸ <public.7yip.QYIPCrawler object at 0x00000171F7365588>

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.7yip.cn/free/?action=china&page1/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.7yip.QYIPCrawler object at 0x00000171F7365588>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.7yip.QYIPCrawler object at 0x00000171F7365588>, 'https://www.7yip.cn/free/?action=china&page1/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 17:34:32.247 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-17 17:42:02.836 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-17 17:49:09.247 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-17 17:55:43.184 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-17 18:01:44.158 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-17 18:07:12.170 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-17 18:11:56.222 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-17 18:16:18.164 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-17 18:20:07.233 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-17 18:23:34.212 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-17 18:26:39.259 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-17 18:29:33.171 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-17 18:32:05.253 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-17 21:28:31.617 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
                 ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
                 ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:28:38.466 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='220.249.149.35', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:28:39.401 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='183.166.96.57', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:28:40.937 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>
        ©¸ Proxy(host='175.43.33.36', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/4'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>, 'http://www.nimadaili.com/gaoni/4')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:29:53.150 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='34.92.94.5', port=8123)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:31:38.036 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='27.43.186.206', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:31:45.800 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='220.249.149.35', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:31:46.776 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='183.166.96.57', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:33:13.135 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='124.156.98.172', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:34:58.840 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='27.43.186.206', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:35:03.918 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='220.249.149.35', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:35:04.963 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='183.166.96.57', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:36:31.259 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='101.200.36.219', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:38:15.634 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='27.43.186.206', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:38:21.195 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='220.249.149.35', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:38:22.161 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='183.166.96.57', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:40:12.638 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='219.146.127.6', port=8060)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:41:58.778 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='27.43.186.206', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:42:05.614 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='220.249.149.35', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:42:06.637 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='183.166.96.57', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:42:35.938 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='112.47.3.53', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='112.47.3.53', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f74db6c8>

IndexError: list index out of range
2020-07-17 21:43:39.252 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='112.47.3.53', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:45:23.780 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='27.43.186.206', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:45:31.071 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='220.249.149.35', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:45:32.058 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='183.166.96.57', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:45:41.140 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='58.87.68.189', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='58.87.68.189', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f74f2f48>

IndexError: list index out of range
2020-07-17 21:46:44.587 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='58.87.68.189', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:48:52.187 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='27.43.186.206', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:48:57.250 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='220.249.149.35', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:48:58.248 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='113.195.17.248', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:49:10.473 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='58.87.68.189', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='58.87.68.189', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f74b4e48>

IndexError: list index out of range
2020-07-17 21:50:13.771 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='58.87.68.189', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:51:58.134 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='27.43.186.206', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:52:05.731 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='220.249.149.35', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:52:06.641 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='113.195.17.248', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:52:24.971 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='58.87.68.189', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='58.87.68.189', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f7472fc8>

IndexError: list index out of range
2020-07-17 21:53:28.226 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='58.87.68.189', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:55:12.131 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='27.43.186.206', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:55:18.884 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='220.249.149.35', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:55:19.830 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='113.195.17.248', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:55:21.505 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>
        ©¸ Proxy(host='222.90.110.194', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/4'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>, 'http://www.nimadaili.com/gaoni/4')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:55:42.934 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='58.87.68.189', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='58.87.68.189', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f74a37c8>

IndexError: list index out of range
2020-07-17 21:56:46.198 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='58.87.68.189', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:58:30.254 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='27.43.186.206', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:58:36.887 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='220.249.149.35', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:58:37.803 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='113.195.17.248', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 21:59:04.754 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='223.242.225.152', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='223.242.225.152', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f74ae4c8>

IndexError: list index out of range
2020-07-17 22:00:08.031 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='223.242.225.152', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:01:54.685 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='1.199.31.52', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:02:00.626 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='220.249.149.35', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:02:01.536 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='113.195.17.248', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:02:02.677 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>
        ©¸ Proxy(host='45.250.226.56', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/3'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>, 'http://www.nimadaili.com/gaoni/3')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:02:21.111 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='223.242.225.152', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='223.242.225.152', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f7472408>

IndexError: list index out of range
2020-07-17 22:03:24.343 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='223.242.225.152', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:05:23.717 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='1.199.31.52', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:05:29.815 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='220.249.149.35', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:05:30.726 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='113.195.17.248', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:05:54.536 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='223.242.225.152', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='223.242.225.152', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f74ba7c8>

IndexError: list index out of range
2020-07-17 22:06:57.763 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='223.242.225.152', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:08:42.426 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='1.199.31.52', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:08:49.590 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='220.249.149.35', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:08:50.528 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='113.195.17.248', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:09:17.676 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='223.242.225.152', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='223.242.225.152', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f74822c8>

IndexError: list index out of range
2020-07-17 22:10:20.900 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='223.242.225.152', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:12:06.788 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='1.199.31.52', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:12:12.196 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='220.249.149.35', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:12:13.098 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='113.195.17.248', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:12:41.463 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='223.242.225.152', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='223.242.225.152', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f7482ec8>

IndexError: list index out of range
2020-07-17 22:13:44.700 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='223.242.225.152', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:15:28.435 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='1.199.31.52', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:15:34.391 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='27.192.172.131', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:15:35.327 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='113.195.17.248', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:15:36.574 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>
        ©¸ Proxy(host='175.43.33.36', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/3'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>, 'http://www.nimadaili.com/gaoni/3')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:15:42.801 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='223.242.225.152', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='223.242.225.152', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f7477288>

IndexError: list index out of range
2020-07-17 22:16:46.067 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='223.242.225.152', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:18:40.160 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='1.199.31.52', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:18:45.032 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='27.192.172.131', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:18:45.992 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='113.195.17.248', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:19:27.864 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='223.242.225.152', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='223.242.225.152', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f74ae748>

IndexError: list index out of range
2020-07-17 22:20:31.110 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='223.242.225.152', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:22:17.278 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='1.199.31.52', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:22:26.622 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='27.192.172.131', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:22:27.671 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='113.195.17.248', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:22:29.251 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>
        ©¸ Proxy(host='222.90.110.194', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/4'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>, 'http://www.nimadaili.com/gaoni/4')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:22:35.390 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='111.206.118.106', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='111.206.118.106', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f74b4808>

IndexError: list index out of range
2020-07-17 22:23:38.620 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='111.206.118.106', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:25:44.879 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='1.199.31.52', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:25:53.834 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='27.192.172.131', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:25:54.794 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='113.195.17.248', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:26:28.927 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='111.206.118.106', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='111.206.118.106', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f74bc208>

IndexError: list index out of range
2020-07-17 22:27:32.207 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='111.206.118.106', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:29:16.539 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='1.199.31.52', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:29:24.721 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='27.192.172.131', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:29:25.701 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='113.195.17.248', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:29:27.280 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>
        ©¸ Proxy(host='175.43.33.36', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/4'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>, 'http://www.nimadaili.com/gaoni/4')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:29:43.233 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='111.206.118.106', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='111.206.118.106', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f7477908>

IndexError: list index out of range
2020-07-17 22:30:46.475 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='111.206.118.106', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:32:40.241 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='27.43.190.12', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:32:45.600 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='27.192.172.131', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:32:46.532 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='113.195.17.248', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:33:39.421 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='111.206.118.106', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='111.206.118.106', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f74d7448>

IndexError: list index out of range
2020-07-17 22:34:42.662 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='111.206.118.106', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:36:29.546 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='27.43.190.12', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:36:37.499 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='27.192.172.131', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:36:38.488 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='113.195.17.248', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:36:39.686 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>
        ©¸ Proxy(host='175.43.33.36', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/3'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>, 'http://www.nimadaili.com/gaoni/3')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:36:57.560 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='111.206.118.106', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='111.206.118.106', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f74ccd48>

IndexError: list index out of range
2020-07-17 22:38:00.838 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='111.206.118.106', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:39:46.858 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='27.43.190.12', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:39:51.905 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='27.192.172.131', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:39:52.824 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='113.195.17.248', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:40:35.200 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='211.137.52.158', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='211.137.52.158', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f7477748>

IndexError: list index out of range
2020-07-17 22:41:38.448 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='211.137.52.158', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:43:22.436 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='27.43.190.12', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:43:28.390 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='27.192.172.131', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:43:29.340 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='113.195.17.248', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:43:30.541 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>
        ©¸ Proxy(host='45.250.226.56', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/3'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>, 'http://www.nimadaili.com/gaoni/3')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:43:40.229 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='211.137.52.158', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='211.137.52.158', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f74baf88>

IndexError: list index out of range
2020-07-17 22:44:43.433 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='211.137.52.158', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:46:50.562 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='27.43.190.12', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:46:58.563 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='27.192.172.131', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:46:59.517 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='115.218.0.53', port='9000')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:47:48.910 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='211.137.52.158', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='211.137.52.158', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f74bc7c8>

IndexError: list index out of range
2020-07-17 22:48:52.159 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='211.137.52.158', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:50:36.499 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='27.43.190.12', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:50:52.009 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='27.192.172.131', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:50:53.023 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='115.218.0.53', port='9000')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:50:54.650 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>
        ©¸ Proxy(host='175.43.33.36', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/4'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x00000171F73657C8>, 'http://www.nimadaili.com/gaoni/4')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:51:13.988 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='118.24.128.46', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000171F70519D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000171F7365808>
        ©¸ Proxy(host='118.24.128.46', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000171F6F71D08>
              ©¸ <Element tr at 0x171f74ba408>

IndexError: list index out of range
2020-07-17 22:52:17.198 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>
        ©¸ Proxy(host='118.24.128.46', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000171F7365848>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:54:08.482 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>
        ©¸ Proxy(host='27.43.190.12', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000171F73655C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:54:15.001 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>
        ©¸ Proxy(host='27.192.172.131', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000171F7365748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000171F7365748>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 22:54:15.964 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (124328), thread 'MainThread' (123648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000171F559F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000171F5579288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000171F55778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000171F734CB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000171F7051AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000171F5575CC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000171F6EC0288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>
        ©¸ Proxy(host='115.218.0.53', port='9000')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000171F6EC01F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000171F7365788>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000171F6EC0168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000171F6A46288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-17 23:09:21.225 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-17 23:21:27.214 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-17 23:32:47.183 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-17 23:43:12.182 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-17 23:52:00.186 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-18 00:00:35.208 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-18 00:08:10.679 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-18 00:14:58.014 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-18 00:20:58.703 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-18 00:26:26.681 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-18 00:31:10.746 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-18 00:35:32.740 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-18 00:39:21.695 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-18 00:42:59.699 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-18 00:46:15.706 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-18 00:49:20.728 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-18 00:52:14.693 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-18 00:54:57.740 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (101636), thread 'MainThread' (124892):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F32A06F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F32A04A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F32A0488B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001F32BE0BE48>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001F32BDD4948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001F32BDCC438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001F32A045D88>

TypeError: 'NoneType' object is not iterable
2020-07-18 07:09:59.295 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>
        ©¸ Proxy(host='27.43.187.53', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:10:05.254 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>
        ©¸ Proxy(host='60.162.46.7', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000136930369C8>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:10:06.355 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>
        ©¸ Proxy(host='118.113.247.182', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:10:24.123 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000013693036A88>
        ©¸ Proxy(host='159.138.21.170', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x0000013692D239D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000013693036A88>
        ©¸ Proxy(host='159.138.21.170', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x0000013692C42D08>
              ©¸ <Element tr at 0x1369315e3c8>

IndexError: list index out of range
2020-07-18 07:11:27.405 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>
        ©¸ Proxy(host='159.138.21.170', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:13:13.231 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>
        ©¸ Proxy(host='27.43.187.53', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:13:18.018 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>
        ©¸ Proxy(host='60.162.46.7', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000136930369C8>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:13:18.934 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>
        ©¸ Proxy(host='118.113.247.182', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:13:21.115 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x0000013693036A48>
        ©¸ Proxy(host='45.76.121.90', port=32202)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/5'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x0000013693036A48>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x0000013693036A48>, 'http://www.nimadaili.com/gaoni/5')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:13:28.592 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000013693036A88>
        ©¸ Proxy(host='159.138.21.170', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x0000013692D239D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000013693036A88>
        ©¸ Proxy(host='159.138.21.170', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x0000013692C42D08>
              ©¸ <Element tr at 0x13693174e08>

IndexError: list index out of range
2020-07-18 07:14:31.881 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>
        ©¸ Proxy(host='159.138.21.170', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:16:16.421 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>
        ©¸ Proxy(host='27.43.187.53', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:16:21.258 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>
        ©¸ Proxy(host='60.162.46.7', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000136930369C8>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:16:44.274 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>
        ©¸ Proxy(host='60.167.102.241', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/3/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>, 'https://www.kuaidaili.com/free/inha/3/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:16:59.329 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000013693036A88>
        ©¸ Proxy(host='23.101.2.247', port=81)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x0000013692D239D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000013693036A88>
        ©¸ Proxy(host='23.101.2.247', port=81)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x0000013692C42D08>
              ©¸ <Element tr at 0x1369317f648>

IndexError: list index out of range
2020-07-18 07:18:02.620 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>
        ©¸ Proxy(host='23.101.2.247', port=81)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:19:47.290 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>
        ©¸ Proxy(host='27.43.187.53', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:19:52.109 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>
        ©¸ Proxy(host='60.162.46.7', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000136930369C8>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:19:53.172 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>
        ©¸ Proxy(host='118.113.247.182', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:19:59.578 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000013693036A88>
        ©¸ Proxy(host='23.101.2.247', port=81)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x0000013692D239D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000013693036A88>
        ©¸ Proxy(host='23.101.2.247', port=81)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x0000013692C42D08>
              ©¸ <Element tr at 0x13693161ac8>

IndexError: list index out of range
2020-07-18 07:21:02.847 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>
        ©¸ Proxy(host='23.101.2.247', port=81)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:23:09.850 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>
        ©¸ Proxy(host='27.43.187.53', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:23:14.725 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>
        ©¸ Proxy(host='60.162.46.7', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000136930369C8>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:23:16.774 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>
        ©¸ Proxy(host='118.113.247.182', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:23:29.812 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000013693036A88>
        ©¸ Proxy(host='23.101.2.247', port=81)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x0000013692D239D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000013693036A88>
        ©¸ Proxy(host='23.101.2.247', port=81)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x0000013692C42D08>
              ©¸ <Element tr at 0x13693103ec8>

IndexError: list index out of range
2020-07-18 07:24:33.061 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>
        ©¸ Proxy(host='23.101.2.247', port=81)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:26:17.283 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>
        ©¸ Proxy(host='27.43.187.53', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:26:43.482 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>
        ©¸ Proxy(host='60.182.19.220', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000136930369C8>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:26:44.452 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>
        ©¸ Proxy(host='118.113.247.182', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:26:46.630 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x0000013693036A48>
        ©¸ Proxy(host='45.76.121.90', port=32202)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/5'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x0000013693036A48>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x0000013693036A48>, 'http://www.nimadaili.com/gaoni/5')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:28:33.548 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>
        ©¸ Proxy(host='210.61.240.158', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:30:19.544 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>
        ©¸ Proxy(host='183.166.111.156', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:30:24.684 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>
        ©¸ Proxy(host='60.182.19.220', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000136930369C8>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:30:25.690 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>
        ©¸ Proxy(host='118.113.247.182', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:31:59.575 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>
        ©¸ Proxy(host='101.200.127.149', port=3129)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:33:43.525 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>
        ©¸ Proxy(host='183.166.111.156', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:33:48.281 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>
        ©¸ Proxy(host='60.182.19.220', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000136930369C8>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:33:49.280 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>
        ©¸ Proxy(host='118.113.247.182', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:33:54.597 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000013693036A88>
        ©¸ Proxy(host='1.64.75.130', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x0000013692D239D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000013693036A88>
        ©¸ Proxy(host='1.64.75.130', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x0000013692C42D08>
              ©¸ <Element tr at 0x1369315e848>

IndexError: list index out of range
2020-07-18 07:34:57.875 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>
        ©¸ Proxy(host='1.64.75.130', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:36:43.577 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>
        ©¸ Proxy(host='183.166.111.156', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:36:48.534 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>
        ©¸ Proxy(host='60.182.19.220', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000136930369C8>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:36:49.552 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>
        ©¸ Proxy(host='118.113.247.182', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:36:51.726 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x0000013693036A48>
        ©¸ Proxy(host='45.76.121.90', port=32202)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/5'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x0000013693036A48>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x0000013693036A48>, 'http://www.nimadaili.com/gaoni/5')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:36:54.712 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000013693036A88>
        ©¸ Proxy(host='60.217.64.237', port=38829)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x0000013692D239D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000013693036A88>
        ©¸ Proxy(host='60.217.64.237', port=38829)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x0000013692C42D08>
              ©¸ <Element tr at 0x13693181688>

IndexError: list index out of range
2020-07-18 07:37:57.967 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>
        ©¸ Proxy(host='60.217.64.237', port=38829)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:39:54.126 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>
        ©¸ Proxy(host='183.166.111.156', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:39:59.196 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>
        ©¸ Proxy(host='60.182.19.220', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000136930369C8>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:40:00.267 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>
        ©¸ Proxy(host='118.113.247.182', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:40:33.146 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000013693036A88>
        ©¸ Proxy(host='60.217.64.237', port=38829)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x0000013692D239D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000013693036A88>
        ©¸ Proxy(host='60.217.64.237', port=38829)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x0000013692C42D08>
              ©¸ <Element tr at 0x13693146688>

IndexError: list index out of range
2020-07-18 07:41:36.409 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>
        ©¸ Proxy(host='60.217.64.237', port=38829)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:43:21.064 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>
        ©¸ Proxy(host='183.166.111.156', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:43:25.973 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>
        ©¸ Proxy(host='60.182.19.220', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000136930369C8>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:43:26.964 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>
        ©¸ Proxy(host='118.113.247.182', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:43:38.921 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000013693036A88>
        ©¸ Proxy(host='60.217.64.237', port=38829)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x0000013692D239D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000013693036A88>
        ©¸ Proxy(host='60.217.64.237', port=38829)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x0000013692C42D08>
              ©¸ <Element tr at 0x13692c1afc8>

IndexError: list index out of range
2020-07-18 07:44:42.190 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>
        ©¸ Proxy(host='60.217.64.237', port=38829)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:46:26.406 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>
        ©¸ Proxy(host='183.166.111.156', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:46:31.423 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>
        ©¸ Proxy(host='60.182.19.220', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000136930369C8>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:46:32.462 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>
        ©¸ Proxy(host='118.113.247.182', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:47:46.760 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>
        ©¸ Proxy(host='223.242.225.220', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:49:56.557 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>
        ©¸ Proxy(host='183.166.111.156', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:50:01.893 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>
        ©¸ Proxy(host='60.182.19.220', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000136930369C8>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:50:02.911 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>
        ©¸ Proxy(host='114.239.172.17', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:51:45.102 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>
        ©¸ Proxy(host='218.59.193.14', port=38640)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:53:29.337 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>
        ©¸ Proxy(host='183.166.111.156', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:53:34.265 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>
        ©¸ Proxy(host='60.182.19.220', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000136930369C8>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:53:35.263 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>
        ©¸ Proxy(host='114.239.172.17', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:53:43.266 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000013693036A88>
        ©¸ Proxy(host='221.224.136.211', port=35101)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x0000013692D239D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000013693036A88>
        ©¸ Proxy(host='221.224.136.211', port=35101)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x0000013692C42D08>
              ©¸ <Element tr at 0x13693106808>

IndexError: list index out of range
2020-07-18 07:54:46.530 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>
        ©¸ Proxy(host='221.224.136.211', port=35101)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:56:31.139 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>
        ©¸ Proxy(host='183.166.111.156', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:56:36.070 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>
        ©¸ Proxy(host='60.182.19.220', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000136930369C8>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:56:37.092 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>
        ©¸ Proxy(host='114.239.172.17', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:56:45.459 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000013693036A88>
        ©¸ Proxy(host='221.224.136.211', port=35101)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x0000013692D239D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000013693036A88>
        ©¸ Proxy(host='221.224.136.211', port=35101)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x0000013692C42D08>
              ©¸ <Element tr at 0x13693103a88>

IndexError: list index out of range
2020-07-18 07:57:48.749 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>
        ©¸ Proxy(host='221.224.136.211', port=35101)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:59:48.827 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>
        ©¸ Proxy(host='183.166.111.156', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:59:57.015 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>
        ©¸ Proxy(host='60.182.19.220', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000136930369C8>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 07:59:58.057 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>
        ©¸ Proxy(host='114.239.172.17', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:00:11.818 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000013693036A88>
        ©¸ Proxy(host='47.104.201.136', port=55443)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x0000013692D239D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000013693036A88>
        ©¸ Proxy(host='47.104.201.136', port=55443)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x0000013692C42D08>
              ©¸ <Element tr at 0x136931585c8>

IndexError: list index out of range
2020-07-18 08:01:15.082 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>
        ©¸ Proxy(host='47.104.201.136', port=55443)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:02:59.416 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>
        ©¸ Proxy(host='182.34.36.105', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:03:25.962 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>
        ©¸ Proxy(host='60.182.19.220', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000136930369C8>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:03:26.962 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>
        ©¸ Proxy(host='114.239.172.17', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:04:41.043 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>
        ©¸ Proxy(host='113.204.164.194', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:06:41.536 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>
        ©¸ Proxy(host='182.34.36.105', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:06:46.346 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>
        ©¸ Proxy(host='60.182.19.220', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000136930369C8>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:06:47.299 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>
        ©¸ Proxy(host='114.239.172.17', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:08:21.534 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>
        ©¸ Proxy(host='222.74.65.86', port=56210)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:10:05.855 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>
        ©¸ Proxy(host='182.34.36.105', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:10:10.842 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>
        ©¸ Proxy(host='60.182.19.220', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000136930369C8>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:10:11.864 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>
        ©¸ Proxy(host='114.239.172.17', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:11:27.342 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>
        ©¸ Proxy(host='159.138.21.170', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:13:25.137 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>
        ©¸ Proxy(host='182.34.36.105', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:13:30.334 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>
        ©¸ Proxy(host='8.210.35.219', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000136930369C8>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:13:31.442 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>
        ©¸ Proxy(host='114.239.172.17', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:13:33.578 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x0000013693036A48>
        ©¸ Proxy(host='175.43.33.36', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/5'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x0000013693036A48>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x0000013693036A48>, 'http://www.nimadaili.com/gaoni/5')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:14:46.719 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>
        ©¸ Proxy(host='118.24.88.240', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:16:31.059 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>
        ©¸ Proxy(host='182.34.36.105', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:16:36.084 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>
        ©¸ Proxy(host='8.210.35.219', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000136930369C8>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:16:37.103 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>
        ©¸ Proxy(host='114.239.172.17', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:17:54.989 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>
        ©¸ Proxy(host='59.188.69.142', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:19:39.394 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>
        ©¸ Proxy(host='182.34.36.105', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:19:44.660 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>
        ©¸ Proxy(host='8.210.35.219', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000136930369C8>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:19:45.648 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>
        ©¸ Proxy(host='114.239.172.17', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:21:00.811 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>
        ©¸ Proxy(host='222.249.238.138', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:23:03.229 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>
        ©¸ Proxy(host='182.34.36.105', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:23:08.168 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>
        ©¸ Proxy(host='8.210.35.219', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000136930369C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000136930369C8>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:23:09.118 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>
        ©¸ Proxy(host='114.239.172.17', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000013693036A08>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:24:25.818 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>
        ©¸ Proxy(host='118.24.127.144', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x0000013693036AC8>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:26:10.749 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (211948), thread 'MainThread' (211952):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000013691280798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001369125A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000136912578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001369301CDC8>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000013692D23AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000013691254E88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000013692B91288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>
        ©¸ Proxy(host='182.34.36.105', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000013692B911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x0000013693036848>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000013692B91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000013692716288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-18 08:44:47.783 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (211928), thread 'MainThread' (211932):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000002A69BA6F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000002A69BA49288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000002A69BA478B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000002A69D80BD88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000002A69D7D2948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000002A69D7CA438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

TypeError: 'NoneType' object is not iterable
2020-07-18 08:57:57.657 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (211928), thread 'MainThread' (211932):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000002A69BA6F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000002A69BA49288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000002A69BA478B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000002A69D80BD88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000002A69D7D2948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000002A69D7CA438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

TypeError: 'NoneType' object is not iterable
2020-07-18 09:10:01.666 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (211928), thread 'MainThread' (211932):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000002A69BA6F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000002A69BA49288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000002A69BA478B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000002A69D80BD88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000002A69D7D2948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000002A69D7CA438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

TypeError: 'NoneType' object is not iterable
2020-07-18 09:20:59.880 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (211928), thread 'MainThread' (211932):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000002A69BA6F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000002A69BA49288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000002A69BA478B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000002A69D80BD88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000002A69D7D2948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000002A69D7CA438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

TypeError: 'NoneType' object is not iterable
2020-07-18 09:31:02.670 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (211928), thread 'MainThread' (211932):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000002A69BA6F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000002A69BA49288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000002A69BA478B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000002A69D80BD88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000002A69D7D2948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000002A69D7CA438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

TypeError: 'NoneType' object is not iterable
2020-07-18 09:39:59.722 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (211928), thread 'MainThread' (211932):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000002A69BA6F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000002A69BA49288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000002A69BA478B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000002A69D80BD88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000002A69D7D2948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000002A69D7CA438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

TypeError: 'NoneType' object is not iterable
2020-07-18 09:48:12.659 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (211928), thread 'MainThread' (211932):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000002A69BA6F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000002A69BA49288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000002A69BA478B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000002A69D80BD88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000002A69D7D2948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000002A69D7CA438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

TypeError: 'NoneType' object is not iterable
2020-07-18 09:55:30.653 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (211928), thread 'MainThread' (211932):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000002A69BA6F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000002A69BA49288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000002A69BA478B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000002A69D80BD88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000002A69D7D2948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000002A69D7CA438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

TypeError: 'NoneType' object is not iterable
2020-07-18 10:02:04.664 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (211928), thread 'MainThread' (211932):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000002A69BA6F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000002A69BA49288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000002A69BA478B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000002A69D80BD88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000002A69D7D2948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000002A69D7CA438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

TypeError: 'NoneType' object is not iterable
2020-07-18 10:08:05.652 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (211928), thread 'MainThread' (211932):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000002A69BA6F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000002A69BA49288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000002A69BA478B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000002A69D80BD88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000002A69D7D2948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000002A69D7CA438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

TypeError: 'NoneType' object is not iterable
2020-07-18 10:13:24.905 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (211928), thread 'MainThread' (211932):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000002A69BA6F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000002A69BA49288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000002A69BA478B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000002A69D80BD88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000002A69D7D2948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000002A69D7CA438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

TypeError: 'NoneType' object is not iterable
2020-07-18 10:18:19.662 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (211928), thread 'MainThread' (211932):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000002A69BA6F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000002A69BA49288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000002A69BA478B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000002A69D80BD88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000002A69D7D2948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000002A69D7CA438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

TypeError: 'NoneType' object is not iterable
2020-07-18 10:22:41.667 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (211928), thread 'MainThread' (211932):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000002A69BA6F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000002A69BA49288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000002A69BA478B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000002A69D80BD88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000002A69D7D2948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000002A69D7CA438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

TypeError: 'NoneType' object is not iterable
2020-07-18 10:26:52.663 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (211928), thread 'MainThread' (211932):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000002A69BA6F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000002A69BA49288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000002A69BA478B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000002A69D80BD88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000002A69D7D2948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000002A69D7CA438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

TypeError: 'NoneType' object is not iterable
2020-07-18 10:30:41.683 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (211928), thread 'MainThread' (211932):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000002A69BA6F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000002A69BA49288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000002A69BA478B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000002A69D80BD88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000002A69D7D2948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000002A69D7CA438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

TypeError: 'NoneType' object is not iterable
2020-07-18 10:34:19.660 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (211928), thread 'MainThread' (211932):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000002A69BA6F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000002A69BA49288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000002A69BA478B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000002A69D80BD88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000002A69D7D2948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000002A69D7CA438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000002A69BA44E48>

TypeError: 'NoneType' object is not iterable
2020-07-20 22:20:04.291 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (57788), thread 'MainThread' (73084):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000203E1CFF798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000203E1CDA288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000203E1CD88B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000203E3A9BA48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000203E37A3AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000203E1CD5D08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000203E3611288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000203E3AB64C8>
        ©¸ Proxy(host='59.62.26.199', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000203E36111F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000203E3AB64C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000203E3AB64C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000203E3611168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000203E3197288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-20 22:20:18.713 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (57788), thread 'MainThread' (73084):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000203E1CFF798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000203E1CDA288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000203E1CD88B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000203E3A9BA48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000203E37A3AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000203E1CD5D08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000203E3611288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x00000203E3AB6648>
        ©¸ Proxy(host='223.241.7.181', port=3000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000203E36111F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x00000203E3AB6648>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x00000203E3AB6648>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000203E3611168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000203E3197288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-20 22:20:19.813 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (57788), thread 'MainThread' (73084):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000203E1CFF798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000203E1CDA288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000203E1CD88B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000203E3A9BA48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000203E37A3AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000203E1CD5D08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000203E3611288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000203E3AB6688>
        ©¸ Proxy(host='113.194.137.35', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000203E36111F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000203E3AB6688>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000203E3AB6688>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000203E3611168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000203E3197288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-20 22:20:59.762 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (57788), thread 'MainThread' (73084):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000203E1CFF798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000203E1CDA288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000203E1CD88B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000203E3A9BA48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000203E37A3AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000203E1CD5D08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000203E3611288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000203E3AB6708>
        ©¸ Proxy(host='112.245.17.202', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x00000203E37A39D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x00000203E3AB6708>
        ©¸ Proxy(host='112.245.17.202', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x00000203E36C2D08>
              ©¸ <Element tr at 0x203e3bd5d48>

IndexError: list index out of range
2020-07-20 22:22:03.200 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (57788), thread 'MainThread' (73084):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000203E1CFF798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000203E1CDA288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000203E1CD88B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000203E3A9BA48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000203E37A3AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000203E1CD5D08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000203E3611288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000203E3AB6748>
        ©¸ Proxy(host='112.245.17.202', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000203E36111F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x00000203E3AB6748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x00000203E3AB6748>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000203E3611168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000203E3197288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-20 22:24:15.916 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (57788), thread 'MainThread' (73084):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000203E1CFF798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000203E1CDA288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000203E1CD88B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000203E3A9BA48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000203E37A3AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000203E1CD5D08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000203E3611288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000203E3AB64C8>
        ©¸ Proxy(host='59.62.26.199', port=9000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000203E36111F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x00000203E3AB64C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x00000203E3AB64C8>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000203E3611168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000203E3197288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-20 22:40:51.423 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (75924), thread 'MainThread' (72400):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001327A8BF798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001327A89A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001327A8988B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001327C65BC88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001327C624948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001327C61C438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

TypeError: 'NoneType' object is not iterable
2020-07-20 22:58:12.448 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (75924), thread 'MainThread' (72400):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001327A8BF798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001327A89A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001327A8988B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001327C65BC88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001327C624948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001327C61C438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

TypeError: 'NoneType' object is not iterable
2020-07-20 23:06:36.290 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (75924), thread 'MainThread' (72400):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001327A8BF798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001327A89A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001327A8988B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001327C65BC88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001327C624948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001327C61C438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

TypeError: 'NoneType' object is not iterable
2020-07-20 23:14:49.348 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (75924), thread 'MainThread' (72400):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001327A8BF798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001327A89A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001327A8988B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001327C65BC88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001327C624948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001327C61C438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

TypeError: 'NoneType' object is not iterable
2020-07-20 23:22:40.382 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (75924), thread 'MainThread' (72400):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001327A8BF798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001327A89A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001327A8988B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001327C65BC88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001327C624948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001327C61C438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

TypeError: 'NoneType' object is not iterable
2020-07-20 23:30:09.325 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (75924), thread 'MainThread' (72400):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001327A8BF798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001327A89A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001327A8988B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001327C65BC88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001327C624948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001327C61C438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

TypeError: 'NoneType' object is not iterable
2020-07-20 23:37:16.415 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (75924), thread 'MainThread' (72400):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001327A8BF798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001327A89A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001327A8988B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001327C65BC88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001327C624948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001327C61C438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

TypeError: 'NoneType' object is not iterable
2020-07-20 23:44:01.183 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (75924), thread 'MainThread' (72400):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001327A8BF798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001327A89A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001327A8988B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001327C65BC88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001327C624948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001327C61C438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

TypeError: 'NoneType' object is not iterable
2020-07-20 23:50:13.411 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (75924), thread 'MainThread' (72400):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001327A8BF798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001327A89A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001327A8988B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001327C65BC88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001327C624948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001327C61C438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

TypeError: 'NoneType' object is not iterable
2020-07-20 23:56:03.404 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (75924), thread 'MainThread' (72400):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001327A8BF798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001327A89A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001327A8988B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001327C65BC88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001327C624948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001327C61C438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

TypeError: 'NoneType' object is not iterable
2020-07-21 00:01:31.325 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (75924), thread 'MainThread' (72400):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001327A8BF798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001327A89A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001327A8988B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001327C65BC88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001327C624948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001327C61C438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

TypeError: 'NoneType' object is not iterable
2020-07-21 00:06:37.421 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (75924), thread 'MainThread' (72400):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001327A8BF798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001327A89A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001327A8988B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001327C65BC88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001327C624948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001327C61C438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

TypeError: 'NoneType' object is not iterable
2020-07-21 00:11:32.231 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (75924), thread 'MainThread' (72400):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001327A8BF798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001327A89A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001327A8988B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001327C65BC88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001327C624948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001327C61C438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

TypeError: 'NoneType' object is not iterable
2020-07-21 00:16:05.319 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (75924), thread 'MainThread' (72400):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001327A8BF798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001327A89A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001327A8988B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001327C65BC88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001327C624948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001327C61C438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

TypeError: 'NoneType' object is not iterable
2020-07-21 00:20:28.390 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (75924), thread 'MainThread' (72400):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001327A8BF798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001327A89A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001327A8988B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001327C65BC88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001327C624948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001327C61C438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

TypeError: 'NoneType' object is not iterable
2020-07-21 00:24:39.370 | ERROR    | proxypool.scheduler:run_tester:37 - An error has been caught in function 'run_tester', process 'Process-1' (75924), thread 'MainThread' (72400):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001327A8BF798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001327A89A288>
           ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001327A8988B8>
    ©¸ <Process(Process-1, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-1, started)>
    ©¦    ©¸ <bound method Scheduler.run_tester of <proxypool.scheduler.Scheduler object at 0x000001327C65BC88>>
    ©¸ <Process(Process-1, started)>

> File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 37, in run_tester
    tester.run()
    ©¦      ©¸ <function Tester.run at 0x000001327C624948>
    ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

  File "f:\spider_projects\proxypool\proxypool\processors\tester.py", line 73, in run
    tasks = [self.test(proxy) for proxy in proxies]
             ©¦    ©¦                        ©¸ None
             ©¦    ©¸ <function Tester.test at 0x000001327C61C438>
             ©¸ <proxypool.processors.tester.Tester object at 0x000001327A896B48>

TypeError: 'NoneType' object is not iterable
2020-07-21 09:55:28.500 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>
        ©¸ Proxy(host='183.167.217.152', port=63000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 09:55:37.448 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>
        ©¸ Proxy(host='36.248.129.92', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x000001F91C637808>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 09:55:38.507 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>
        ©¸ Proxy(host='114.99.130.16', port='1133')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 09:56:00.682 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001F91C6378C8>
        ©¸ Proxy(host='222.184.7.206', port=54832)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000001F91C3219D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001F91C6378C8>
        ©¸ Proxy(host='222.184.7.206', port=54832)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000001F91C242D08>
              ©¸ <Element tr at 0x1f91c75cdc8>

IndexError: list index out of range
2020-07-21 09:57:03.954 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>
        ©¸ Proxy(host='222.184.7.206', port=54832)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 09:58:48.704 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>
        ©¸ Proxy(host='183.167.217.152', port=63000)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 09:58:57.636 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>
        ©¸ Proxy(host='36.248.129.92', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x000001F91C637808>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 09:58:58.679 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>
        ©¸ Proxy(host='114.99.130.16', port='1133')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 09:59:10.050 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001F91C6378C8>
        ©¸ Proxy(host='118.24.172.149', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000001F91C3219D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001F91C6378C8>
        ©¸ Proxy(host='118.24.172.149', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000001F91C242D08>
              ©¸ <Element tr at 0x1f91c75cd88>

IndexError: list index out of range
2020-07-21 10:00:13.387 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>
        ©¸ Proxy(host='118.24.172.149', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:02:46.620 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>
        ©¸ Proxy(host='58.22.177.75', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:02:55.594 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>
        ©¸ Proxy(host='36.248.129.92', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x000001F91C637808>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:02:56.597 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>
        ©¸ Proxy(host='114.99.130.16', port='1133')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:03:07.980 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001F91C6378C8>
        ©¸ Proxy(host='118.24.172.149', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000001F91C3219D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001F91C6378C8>
        ©¸ Proxy(host='118.24.172.149', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000001F91C242D08>
              ©¸ <Element tr at 0x1f91c761f08>

IndexError: list index out of range
2020-07-21 10:04:11.307 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>
        ©¸ Proxy(host='118.24.172.149', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:05:56.186 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>
        ©¸ Proxy(host='58.22.177.75', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:06:02.145 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>
        ©¸ Proxy(host='36.248.129.92', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x000001F91C637808>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:06:03.194 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>
        ©¸ Proxy(host='114.99.130.16', port='1133')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:06:05.335 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x000001F91C637888>
        ©¸ Proxy(host='117.131.119.116', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/5'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x000001F91C637888>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x000001F91C637888>, 'http://www.nimadaili.com/gaoni/5')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:06:08.478 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001F91C6378C8>
        ©¸ Proxy(host='39.106.223.134', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000001F91C3219D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001F91C6378C8>
        ©¸ Proxy(host='39.106.223.134', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000001F91C242D08>
              ©¸ <Element tr at 0x1f91c75cfc8>

IndexError: list index out of range
2020-07-21 10:07:11.748 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>
        ©¸ Proxy(host='39.106.223.134', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:09:01.363 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>
        ©¸ Proxy(host='58.22.177.75', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:09:10.659 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>
        ©¸ Proxy(host='36.248.129.92', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x000001F91C637808>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:09:11.646 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>
        ©¸ Proxy(host='114.99.130.16', port='1133')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:09:24.740 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001F91C6378C8>
        ©¸ Proxy(host='39.106.223.134', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000001F91C3219D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001F91C6378C8>
        ©¸ Proxy(host='39.106.223.134', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000001F91C242D08>
              ©¸ <Element tr at 0x1f91c772548>

IndexError: list index out of range
2020-07-21 10:10:28.030 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>
        ©¸ Proxy(host='39.106.223.134', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:12:12.788 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>
        ©¸ Proxy(host='58.22.177.75', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:12:21.012 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>
        ©¸ Proxy(host='36.248.129.92', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x000001F91C637808>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:12:22.076 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>
        ©¸ Proxy(host='114.99.130.16', port='1133')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:12:24.348 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x000001F91C637888>
        ©¸ Proxy(host='75.80.242.9', port=41007)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/5'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x000001F91C637888>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x000001F91C637888>, 'http://www.nimadaili.com/gaoni/5')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:12:32.456 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001F91C6378C8>
        ©¸ Proxy(host='39.106.223.134', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000001F91C3219D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001F91C6378C8>
        ©¸ Proxy(host='39.106.223.134', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000001F91C242D08>
              ©¸ <Element tr at 0x1f91c76fec8>

IndexError: list index out of range
2020-07-21 10:13:35.743 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>
        ©¸ Proxy(host='39.106.223.134', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:15:20.962 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>
        ©¸ Proxy(host='58.22.177.75', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:15:29.114 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>
        ©¸ Proxy(host='36.248.129.92', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x000001F91C637808>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:15:30.064 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>
        ©¸ Proxy(host='114.99.130.16', port='1133')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:15:31.962 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x000001F91C637888>
        ©¸ Proxy(host='116.58.232.220', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/4'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x000001F91C637888>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x000001F91C637888>, 'http://www.nimadaili.com/gaoni/4')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:15:45.378 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001F91C6378C8>
        ©¸ Proxy(host='39.106.223.134', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000001F91C3219D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001F91C6378C8>
        ©¸ Proxy(host='39.106.223.134', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000001F91C242D08>
              ©¸ <Element tr at 0x1f91c760b88>

IndexError: list index out of range
2020-07-21 10:16:48.668 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>
        ©¸ Proxy(host='39.106.223.134', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:18:47.357 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>
        ©¸ Proxy(host='58.22.177.75', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:18:58.625 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>
        ©¸ Proxy(host='36.248.129.92', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x000001F91C637808>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:18:59.719 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>
        ©¸ Proxy(host='114.99.130.16', port='1133')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:19:30.263 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001F91C6378C8>
        ©¸ Proxy(host='39.106.223.134', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000001F91C3219D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001F91C6378C8>
        ©¸ Proxy(host='39.106.223.134', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000001F91C242D08>
              ©¸ <Element tr at 0x1f91c748a88>

IndexError: list index out of range
2020-07-21 10:20:33.673 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>
        ©¸ Proxy(host='39.106.223.134', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:22:18.231 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>
        ©¸ Proxy(host='58.22.177.75', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:22:46.251 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>
        ©¸ Proxy(host='36.248.129.92', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x000001F91C637808>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:22:47.326 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>
        ©¸ Proxy(host='114.99.130.16', port='1133')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:22:49.788 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x000001F91C637888>
        ©¸ Proxy(host='125.110.206.69', port=47522)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/5'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x000001F91C637888>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x000001F91C637888>, 'http://www.nimadaili.com/gaoni/5')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:24:14.962 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>
        ©¸ Proxy(host='103.94.180.171', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:25:59.482 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>
        ©¸ Proxy(host='58.22.177.75', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:26:05.560 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>
        ©¸ Proxy(host='36.248.129.92', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x000001F91C637808>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:26:06.616 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>
        ©¸ Proxy(host='114.99.130.16', port='1133')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:27:26.251 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>
        ©¸ Proxy(host='139.217.110.76', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:29:11.250 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>
        ©¸ Proxy(host='58.22.177.75', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:29:24.297 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>
        ©¸ Proxy(host='36.248.129.92', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x000001F91C637808>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:29:25.404 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>
        ©¸ Proxy(host='114.99.130.16', port='1133')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:30:48.564 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>
        ©¸ Proxy(host='222.89.32.175', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:32:33.223 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>
        ©¸ Proxy(host='218.14.15.222', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:32:51.295 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>
        ©¸ Proxy(host='36.248.129.92', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x000001F91C637808>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:32:52.325 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>
        ©¸ Proxy(host='114.99.130.16', port='1133')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:34:31.571 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>
        ©¸ Proxy(host='101.37.118.54', port=8888)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:36:41.056 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>
        ©¸ Proxy(host='218.14.15.222', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:36:50.811 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>
        ©¸ Proxy(host='183.166.162.173', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x000001F91C637808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x000001F91C637808>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:36:51.924 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>
        ©¸ Proxy(host='114.99.130.16', port='1133')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001F91C637848>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:37:31.162 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001F91C6378C8>
        ©¸ Proxy(host='58.87.98.150', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000001F91C3219D8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001F91C6378C8>
        ©¸ Proxy(host='58.87.98.150', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000001F91C242D08>
              ©¸ <Element tr at 0x1f91c777e48>

IndexError: list index out of range
2020-07-21 10:38:34.499 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>
        ©¸ Proxy(host='58.87.98.150', port=1080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001F91C637908>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-21 10:40:19.721 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (90432), thread 'MainThread' (89144):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F91A87F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F91A85A288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F91A8588B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F91C61BC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F91C321AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F91A855E08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F91C191288>
        ©¦        ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>
        ©¸ Proxy(host='218.14.15.222', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://cn-proxy.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F91C1911F8>
           ©¸ <public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.cn_proxy.CnProxyCrawler object at 0x000001F91C637688>, 'http://cn-proxy.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F91C191168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F91BD17288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 17:14:41.308 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (25904), thread 'MainThread' (25908):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000018975F7F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x0000018975F59288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x0000018975F578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x0000018977D1AC48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000018977A21AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000018975F55BC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
                 ©¦       ©¸ <function BaseCrawler.crawl at 0x0000018977891288>
                 ©¸ <public.7yip.QYIPCrawler object at 0x0000018977D35688>

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.7yip.cn/free/?action=china&page1/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000189778911F8>
           ©¸ <public.7yip.QYIPCrawler object at 0x0000018977D35688>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.7yip.QYIPCrawler object at 0x0000018977D35688>, 'https://www.7yip.cn/free/?action=china&page1/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000018977891168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000018977418288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 17:18:30.834 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (26488), thread 'MainThread' (16816):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001BED765F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001BED7639288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001BED76378B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001BED93FCC88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001BED9102AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001BED7B00448>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
                 ©¦       ©¸ <function BaseCrawler.crawl at 0x000001BED8F71288>
                 ©¸ <public.7yip.QYIPCrawler object at 0x000001BED94165C8>

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.7yip.cn/free/?action=china&page=1/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001BED8F711F8>
           ©¸ <public.7yip.QYIPCrawler object at 0x000001BED94165C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.7yip.QYIPCrawler object at 0x000001BED94165C8>, 'https://www.7yip.cn/free/?action=china&page=1/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001BED8F71168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001BED8AF7288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 17:20:45.937 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (27280), thread 'MainThread' (27020):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001F61525F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001F615239288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001F6152378B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001F616FFBB48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001F616D01AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001F615235A08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
                 ©¦       ©¸ <function BaseCrawler.crawl at 0x000001F616B71288>
                 ©¸ <public.7yip.QYIPCrawler object at 0x000001F617016588>

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.7yip.cn/free/?action=china&page=1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001F616B711F8>
           ©¸ <public.7yip.QYIPCrawler object at 0x000001F617016588>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.7yip.QYIPCrawler object at 0x000001F617016588>, 'https://www.7yip.cn/free/?action=china&page=1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001F616B71168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001F6166F7288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 17:22:11.146 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (26792), thread 'MainThread' (25744):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D33450F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D3344E8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D3344E78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D3362ABC08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D335FB2AF8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D3344E5C08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
                 ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D335E21288>
                 ©¸ <public.7yip.QYIPCrawler object at 0x000001D3362C65C8>

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.7yip.cn/free/?action=china&page=1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D335E211F8>
           ©¸ <public.7yip.QYIPCrawler object at 0x000001D3362C65C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.7yip.QYIPCrawler object at 0x000001D3362C65C8>, 'https://www.7yip.cn/free/?action=china&page=1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D335E21168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D3359A8288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 17:25:48.749 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (26936), thread 'MainThread' (26344):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000022D5B60F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x0000022D5B5E9288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x0000022D5B5E78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x0000022D5D3BC108>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000022D5D0B21F8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000022D5B5E4D88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000022D5CF22288>
        ©¦        ©¸ <public.iphai.IPHaiCrawler object at 0x0000022D5D3C4C08>
        ©¸ Proxy(host='36.248.133.229', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.iphai.com/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000022D5CF221F8>
           ©¸ <public.iphai.IPHaiCrawler object at 0x0000022D5D3C4C08>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.iphai.IPHaiCrawler object at 0x0000022D5D3C4C08>, 'http://www.iphai.com/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000022D5CF22168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000022D5CAA7288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 17:25:49.587 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (26936), thread 'MainThread' (26344):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000022D5B60F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x0000022D5B5E9288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x0000022D5B5E78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x0000022D5D3BC108>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000022D5D0B21F8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000022D5B5E4D88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000022D5CF22288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000022D5D3C4C88>
        ©¸ Proxy(host='36.248.133.229', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/1/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000022D5CF221F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000022D5D3C4C88>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000022D5D3C4C88>, 'https://www.kuaidaili.com/free/inha/1/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000022D5CF22168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000022D5CAA7288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 17:25:52.751 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (26936), thread 'MainThread' (26344):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000022D5B60F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x0000022D5B5E9288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x0000022D5B5E78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x0000022D5D3BC108>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000022D5D0B21F8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000022D5B5E4D88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000022D5CF22288>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x0000022D5D3C4CC8>
        ©¸ Proxy(host='91.201.240.243', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/5'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000022D5CF221F8>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x0000022D5D3C4CC8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x0000022D5D3C4CC8>, 'http://www.nimadaili.com/gaoni/5')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000022D5CF22168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000022D5CAA7288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 17:25:55.870 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (26936), thread 'MainThread' (26344):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000022D5B60F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x0000022D5B5E9288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x0000022D5B5E78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x0000022D5D3BC108>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000022D5D0B21F8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000022D5B5E4D88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000022D5CF22288>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000022D5D3C4D48>
        ©¸ Proxy(host='207.244.248.208', port=8799)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://ip.ihuan.me/address/5Lit5Zu9.html?page=b97827cc'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000022D5CF221F8>
           ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000022D5D3C4D48>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x0000022D5D3C4D48>, 'https://ip.ihuan.me/address/5Lit5Zu9.html?page=b...
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000022D5CF22168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000022D5CAA7288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 17:30:00.374 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (28584), thread 'MainThread' (16432):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000026C5E00F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x0000026C5DFE9288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x0000026C5DFE78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x0000026C5FDA7D08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000026C5FAACDC8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000026C5DFE4D48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000026C5F920288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000026C5FDC1788>
        ©¸ Proxy(host='220.249.149.89', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/1/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000026C5F9201F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000026C5FDC1788>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000026C5FDC1788>, 'https://www.kuaidaili.com/free/inha/1/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000026C5F920168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000026C5F4A5288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 17:30:47.157 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (27428), thread 'MainThread' (26152):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001B6F955F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001B6F9539288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001B6F95378B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001B6FB2F7B08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001B6FAFFCDC8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001B6F9534D88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001B6FAE71288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001B6FB312608>
        ©¸ Proxy(host='220.249.149.89', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/1/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001B6FAE711F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001B6FB312608>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001B6FB312608>, 'https://www.kuaidaili.com/free/inha/1/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001B6FAE71168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001B6FA9F8288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 17:38:46.311 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (29064), thread 'MainThread' (28644):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001A470F2F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001A470F08288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001A470F078B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001A472CC2B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001A4729C8E58>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001A470F05BC8>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001A472841288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001A472CDD6C8>
        ©¸ Proxy(host='220.249.149.89', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/1/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001A4728411F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001A472CDD6C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001A472CDD6C8>, 'https://www.kuaidaili.com/free/inha/1/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001A472841168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001A4723C7288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 17:53:20.091 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (25260), thread 'MainThread' (26544):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000206CD47F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000206CD459288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000206CD4578B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000206CF212B08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000206CEF17DC8>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000206CD455C88>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000206CED91288>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000206CF22D648>
        ©¸ Proxy(host='220.249.149.89', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 34, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x00000206CED911F8>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000206CF22D648>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x00000206CF22D648>, 'https://www.kuaidaili.com/free/inha/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x00000206CED91168>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x00000206CE917288>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 17:56:40.179 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (33180), thread 'MainThread' (28108):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x00000119A372F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x00000119A3709288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x00000119A37078B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x00000119A54C4A48>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x00000119A51C9E58>
    ©¸ <proxypool.processors.getter.Getter object at 0x00000119A3705C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x00000119A5041318>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000119A54DC588>
        ©¸ Proxy(host='220.249.149.89', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 36, in crawl
    for proxy in self.parse(html):
                 ©¦    ©¦     ©¸ '<!DOCTYPE html>\n<html>\n<head>\n<meta http-equiv="X-UA-Compatible" content="IE=edge" />\n<meta http-equiv="Content-Type" co...
                 ©¦    ©¸ <function KuaidailiCrawler.parse at 0x00000119A51C0EE8>
                 ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x00000119A54DC588>

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\kuaidaili.py", line 29, in parse
    ip = tr.xpath('./td[@data-title="IP"]/text()')[0]
         ©¦  ©¸ <cyfunction _Element.xpath at 0x00000119A514CAC8>
         ©¸ <Element tr at 0x119a555b6c8>

IndexError: list index out of range
2020-07-22 18:01:28.746 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (33540), thread 'MainThread' (29128):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x0000023BE5E9F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x0000023BE5E79288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x0000023BE5E778B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x0000023BE7C34A08>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x0000023BE7938EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x0000023BE5E75C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x0000023BE77B0318>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000023BE7C4D508>
        ©¸ Proxy(host='180.118.128.76', port='9000')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x0000023BE77B0288>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x0000023BE7C4D508>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x0000023BE7C4D508>, 'https://www.kuaidaili.com/free/inha/2')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x0000023BE77B01F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x0000023BE7338318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 18:42:08.832 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (36912), thread 'MainThread' (32784):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000002C7FFED0798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000002C7FFEA9288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000002C7FFEA78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000002C781C95A88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000002C781999EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000002C7FFEA5B08>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000002C781810318>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000002C781CAD548>
        ©¸ Proxy(host='180.118.128.76', port='9000')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000002C781810288>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000002C781CAD548>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000002C781CAD548>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000002C7818101F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000002C781397318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 18:55:52.493 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>
        ©¸ Proxy(host='180.118.128.76', port='9000')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 18:55:54.453 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x000001D55DBCD748>
        ©¸ Proxy(host='101.4.136.34', port=81)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/4'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x000001D55DBCD748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x000001D55DBCD748>, 'http://www.nimadaili.com/gaoni/4')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 18:56:04.687 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001D55DBCD7C8>
        ©¸ Proxy(host='211.21.120.163', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 36, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000001D55D8B9DC8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001D55DBCD7C8>
        ©¸ Proxy(host='211.21.120.163', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000001D55D83CAC8>
              ©¸ <Element tr at 0x1d55dc89f88>

IndexError: list index out of range
2020-07-22 18:57:08.027 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>
        ©¸ Proxy(host='211.21.120.163', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 18:59:28.454 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>
        ©¸ Proxy(host='113.194.29.185', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 18:59:54.836 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001D55DBCD7C8>
        ©¸ Proxy(host='124.172.232.49', port=8010)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 36, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000001D55D8B9DC8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001D55DBCD7C8>
        ©¸ Proxy(host='124.172.232.49', port=8010)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000001D55D83CAC8>
              ©¸ <Element tr at 0x1d55dc81fc8>

IndexError: list index out of range
2020-07-22 19:00:58.142 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>
        ©¸ Proxy(host='124.172.232.49', port=8010)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 19:36:51.261 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>
        ©¸ Proxy(host='113.194.29.185', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 19:36:59.245 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001D55DBCD7C8>
        ©¸ Proxy(host='101.132.39.115', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 36, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000001D55D8B9DC8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001D55DBCD7C8>
        ©¸ Proxy(host='101.132.39.115', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000001D55D83CAC8>
              ©¸ <Element tr at 0x1d55dc8c4c8>

IndexError: list index out of range
2020-07-22 19:38:02.605 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>
        ©¸ Proxy(host='101.132.39.115', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 19:40:13.335 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>
        ©¸ Proxy(host='113.194.29.185', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 19:40:58.943 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001D55DBCD7C8>
        ©¸ Proxy(host='101.132.39.115', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 36, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000001D55D8B9DC8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001D55DBCD7C8>
        ©¸ Proxy(host='101.132.39.115', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000001D55D83CAC8>
              ©¸ <Element tr at 0x1d55dc927c8>

IndexError: list index out of range
2020-07-22 19:42:02.264 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>
        ©¸ Proxy(host='101.132.39.115', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 19:44:23.912 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>
        ©¸ Proxy(host='113.194.29.185', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 19:44:53.862 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001D55DBCD7C8>
        ©¸ Proxy(host='101.132.39.115', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 36, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000001D55D8B9DC8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001D55DBCD7C8>
        ©¸ Proxy(host='101.132.39.115', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000001D55D83CAC8>
              ©¸ <Element tr at 0x1d55dc941c8>

IndexError: list index out of range
2020-07-22 19:45:57.208 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>
        ©¸ Proxy(host='101.132.39.115', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 19:47:45.842 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>
        ©¸ Proxy(host='113.194.29.185', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 19:47:48.413 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x000001D55DBCD748>
        ©¸ Proxy(host='182.23.103.6', port=3137)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/4'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x000001D55DBCD748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x000001D55DBCD748>, 'http://www.nimadaili.com/gaoni/4')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 19:48:02.240 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001D55DBCD7C8>
        ©¸ Proxy(host='101.132.39.115', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 36, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000001D55D8B9DC8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001D55DBCD7C8>
        ©¸ Proxy(host='101.132.39.115', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000001D55D83CAC8>
              ©¸ <Element tr at 0x1d55dc95448>

IndexError: list index out of range
2020-07-22 19:49:05.579 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>
        ©¸ Proxy(host='101.132.39.115', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 19:50:55.391 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>
        ©¸ Proxy(host='113.194.29.185', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 19:51:24.394 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001D55DBCD7C8>
        ©¸ Proxy(host='101.132.39.115', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 36, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000001D55D8B9DC8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001D55DBCD7C8>
        ©¸ Proxy(host='101.132.39.115', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000001D55D83CAC8>
              ©¸ <Element tr at 0x1d55dc8c148>

IndexError: list index out of range
2020-07-22 19:52:27.700 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>
        ©¸ Proxy(host='101.132.39.115', port=8080)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 19:54:34.494 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>
        ©¸ Proxy(host='120.83.101.166', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/3/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>, 'https://www.kuaidaili.com/free/inha/3/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 19:55:04.096 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001D55DBCD7C8>
        ©¸ Proxy(host='139.217.110.76', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 36, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000001D55D8B9DC8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001D55DBCD7C8>
        ©¸ Proxy(host='139.217.110.76', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000001D55D83CAC8>
              ©¸ <Element tr at 0x1d55dc4f848>

IndexError: list index out of range
2020-07-22 19:56:07.495 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>
        ©¸ Proxy(host='139.217.110.76', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 19:57:55.430 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>
        ©¸ Proxy(host='223.242.225.227', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 19:58:02.788 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001D55DBCD7C8>
        ©¸ Proxy(host='139.217.110.76', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 36, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000001D55D8B9DC8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001D55DBCD7C8>
        ©¸ Proxy(host='139.217.110.76', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000001D55D83CAC8>
              ©¸ <Element tr at 0x1d55dc96508>

IndexError: list index out of range
2020-07-22 19:59:06.105 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>
        ©¸ Proxy(host='139.217.110.76', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 20:01:00.023 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>
        ©¸ Proxy(host='223.242.225.227', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 20:01:20.371 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001D55DBCD7C8>
        ©¸ Proxy(host='139.217.110.76', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 36, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000001D55D8B9DC8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001D55DBCD7C8>
        ©¸ Proxy(host='139.217.110.76', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000001D55D83CAC8>
              ©¸ <Element tr at 0x1d55dc9aac8>

IndexError: list index out of range
2020-07-22 20:02:23.623 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>
        ©¸ Proxy(host='139.217.110.76', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 20:04:12.609 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>
        ©¸ Proxy(host='223.242.225.227', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 20:04:14.818 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.nimadaili.NimadailiCrawler object at 0x000001D55DBCD748>
        ©¸ Proxy(host='113.124.95.40', port=9999)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'http://www.nimadaili.com/gaoni/5'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.nimadaili.NimadailiCrawler object at 0x000001D55DBCD748>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.nimadaili.NimadailiCrawler object at 0x000001D55DBCD748>, 'http://www.nimadaili.com/gaoni/5')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 20:04:27.805 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001D55DBCD7C8>
        ©¸ Proxy(host='139.217.110.76', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 36, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000001D55D8B9DC8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001D55DBCD7C8>
        ©¸ Proxy(host='139.217.110.76', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000001D55D83CAC8>
              ©¸ <Element tr at 0x1d55dc94c08>

IndexError: list index out of range
2020-07-22 20:05:31.056 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>
        ©¸ Proxy(host='139.217.110.76', port=3128)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 20:07:18.677 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>
        ©¸ Proxy(host='223.242.225.227', port='9999')

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.kuaidaili.com/free/inha/2/'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.kuaidaili.KuaidailiCrawler object at 0x000001D55DBCD6C8>, 'https://www.kuaidaili.com/free/inha/2/')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
2020-07-22 20:07:32.007 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001D55DBCD7C8>
        ©¸ Proxy(host='211.144.213.145', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 36, in crawl
    for proxy in self.parse(html):
        ©¦        ©¦    ©¦     ©¸ '<!DOCTYPE html><html lang="zh-cn"><head></head><body><meta"utf-8"/><meta name="viewport"content="width=device-width, initial...
        ©¦        ©¦    ©¸ <function XiaohuanProxyCrawler.parse at 0x000001D55D8B9DC8>
        ©¦        ©¸ <public.xiaohuan_proxy.XiaohuanProxyCrawler object at 0x000001D55DBCD7C8>
        ©¸ Proxy(host='211.144.213.145', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\public\xiaohuan_proxy.py", line 29, in parse
    address = tr.xpath('./td[1]/a/text()')[0]
              ©¦  ©¸ <cyfunction _Element.xpath at 0x000001D55D83CAC8>
              ©¸ <Element tr at 0x1d55dc81b48>

IndexError: list index out of range
2020-07-22 20:08:35.352 | ERROR    | proxypool.processors.getter:run:41 - An error has been caught in function 'run', process 'Process-2' (40616), thread 'MainThread' (40648):
Traceback (most recent call last):
  File "<string>", line 1, in <module>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 105, in spawn_main
    exitcode = _main(fd)
               ©¦     ©¸ 3
               ©¸ <function _main at 0x000001D55BE1F798>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\spawn.py", line 118, in _main
    return self._bootstrap()
           ©¦    ©¸ <function BaseProcess._bootstrap at 0x000001D55BDF8288>
           ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 297, in _bootstrap
    self.run()
    ©¦    ©¸ <function BaseProcess.run at 0x000001D55BDF78B8>
    ©¸ <Process(Process-2, started)>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\multiprocessing\process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
    ©¦    ©¦        ©¦    ©¦        ©¦    ©¸ {}
    ©¦    ©¦        ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¦        ©¦    ©¸ ()
    ©¦    ©¦        ©¸ <Process(Process-2, started)>
    ©¦    ©¸ <bound method Scheduler.run_getter of <proxypool.scheduler.Scheduler object at 0x000001D55DBB5B88>>
    ©¸ <Process(Process-2, started)>

  File "f:\spider_projects\proxypool\proxypool\scheduler.py", line 52, in run_getter
    getter.run()
    ©¦      ©¸ <function Getter.run at 0x000001D55D8B9EE8>
    ©¸ <proxypool.processors.getter.Getter object at 0x000001D55BDF5C48>

> File "f:\spider_projects\proxypool\proxypool\processors\getter.py", line 41, in run
    for proxy in crawler.crawl():
        ©¦        ©¦       ©¸ <function BaseCrawler.crawl at 0x000001D55D731318>
        ©¦        ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>
        ©¸ Proxy(host='211.144.213.145', port=80)

  File "f:\spider_projects\proxypool\proxypool\crawlers\base.py", line 35, in crawl
    html = self.fetch(url)
           ©¦    ©¦     ©¸ 'https://www.xicidaili.com/nn/1'
           ©¦    ©¸ <function BaseCrawler.fetch at 0x000001D55D731288>
           ©¸ <public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 49, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ©¦         ©¦        ©¦         ©¦   ©¦       ©¸ {}
           ©¦         ©¦        ©¦         ©¦   ©¸ (<public.xicidaili.XicidailiCrawler object at 0x000001D55DBCD808>, 'https://www.xicidaili.com/nn/1')
           ©¦         ©¦        ©¦         ©¸ <function BaseCrawler.fetch at 0x000001D55D7311F8>
           ©¦         ©¦        ©¸ {'stop_max_attempt_number': 3, 'retry_on_result': <function BaseCrawler.<lambda> at 0x000001D55D2B8318>}
           ©¦         ©¸ ()
           ©¸ <class 'retrying.Retrying'>

  File "D:\ProgramData\Anaconda3\envs\spider\lib\site-packages\retrying.py", line 214, in call
    raise RetryError(attempt)
          ©¦          ©¸ Attempts: 3, Value: None
          ©¸ <class 'retrying.RetryError'>

retrying.RetryError: RetryError[Attempts: 3, Value: None]
